{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gymCopy_qtable.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOA9D9ud79y9O1jEjxr8QAx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaaaxiaofei/notebooks/blob/main/gymCopy/gymCopy_qtable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-858tXksclHe"
      },
      "source": [
        "# Gym Copy-v0 Environment\n",
        "\n",
        "Conslusion: qTable is not a good approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNmt4FDRMdAD"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXkJl8tDMgZs"
      },
      "source": [
        "env = gym.make(\"Copy-v0\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkYErKfxMqye"
      },
      "source": [
        "# Get scalar action_size\n",
        "action_size = 1\n",
        "action_scale = list()\n",
        "for i in range(len(env.action_space)):\n",
        "  local_size = env.action_space[i].n\n",
        "  action_size *= local_size\n",
        "\n",
        "  action_scale = [x*local_size for x in action_scale] if action_scale else []\n",
        "  action_scale.append(1)\n",
        "\n",
        "# Get state size\n",
        "state_size = env.observation_space.n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FudFfKGkMsH_"
      },
      "source": [
        "qtable = np.zeros((state_size, action_size))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukFLq0X7PnOj"
      },
      "source": [
        "def decode_action(action_tuple):\n",
        "  return sum(tuple(x*y for x, y in zip(action_tuple, action_scale)))\n",
        "\n",
        "def encode_action(action_scalar):\n",
        "  num1 = action_scalar//action_scale[0]\n",
        "  num2 = (action_scalar - num1*action_scale[0])//action_scale[1]\n",
        "  num3 = action_scalar - num1*action_scale[0] - num2*action_scale[1]\n",
        "  return (num1, num2, num3)"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmNR2IVDSScR"
      },
      "source": [
        "# Test decode_action and encode_action:\n",
        "action1 = (0, 0, 0)\n",
        "action2 = (1,1,1)\n",
        "action3 = (1, 1, 4)\n",
        "\n",
        "assert(decode_action(action1) == 0)\n",
        "assert(decode_action(action2) == 10 + 5 + 1)\n",
        "assert(decode_action(action3) == 10 + 5 + 4)\n",
        "\n",
        "assert(encode_action(0) == action1)\n",
        "assert(encode_action(16) == action2)\n",
        "assert(encode_action(19) == action3)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUtez0JEO8nR",
        "outputId": "50c7cb2e-97fe-44ea-8538-6243bbf2c09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(env.action_space.sample())"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pi1A7KyM90d",
        "outputId": "f4468006-7f5d-4a9c-fcb2-ede7a66be527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "qtable"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W49IscjOdTK"
      },
      "source": [
        "total_episodes = 15000        # Total episodes\n",
        "learning_rate = 0.8           # Learning rate\n",
        "max_steps = 99                # Max steps per episode\n",
        "gamma = 0.95                  # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.005             # Exponential decay rate for exploration prob"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEV-4mEbOmAm",
        "outputId": "358ad5fe-bbb6-43a5-fd62-5d5e6b4d7207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "rewards = []\n",
        "\n",
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0, 1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = encode_action(np.argmax(qtable[state,:]))\n",
        "\n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, info = env.step(action)\n",
        "\n",
        "        action_scalar = decode_action(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        # qtable[new_state,:] : all the actions we can take from new state\n",
        "        qtable[state, action_scalar] = qtable[state, action_scalar] +\\\n",
        "          learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action_scalar])\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        # If done (if we're dead) : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "        \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
        "    rewards.append(total_rewards)\n",
        "\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
        "print(qtable)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time: 0.5938\n",
            "[[ 4.48644112e+00  2.85822470e+00  5.73360197e+00  2.58433858e-01\n",
            "  -5.69937576e-02 -3.06816970e-01  3.25837307e+00  2.60613213e-01\n",
            "   1.95905120e-01  1.59945677e-01 -1.42933770e-01  2.98318009e-01\n",
            "   5.11389144e-04 -6.13221291e-02 -1.83340203e-01  2.10646081e-01\n",
            "   2.83302596e+00  1.34180924e-01  1.29262970e-01 -1.03448809e-01]\n",
            " [-4.55983127e-01  1.18070034e+00  3.12742515e-01 -7.37025277e-01\n",
            "   3.89100928e+00  2.54828838e-01  6.83800635e+00 -2.76869129e-01\n",
            "  -2.66157410e-01  3.94336560e-02 -4.47049518e-02 -1.83567408e-01\n",
            "  -1.45756091e-01 -7.61042525e-02 -9.46822478e-02 -5.17270063e-02\n",
            "   2.22457684e-01 -9.84306391e-02  3.36056869e-01  2.46087641e-01]\n",
            " [-2.16641992e-01  3.10664723e+00 -1.81055726e-01  3.30976085e-02\n",
            "   3.13678718e-01  2.36780302e-01  2.86317804e-01  2.89952479e-01\n",
            "  -1.00186971e-02  4.88609946e+00 -2.15191408e-02  2.98803817e-01\n",
            "   4.01305797e-01  4.59144342e+00  1.45072884e-01  3.00726391e+00\n",
            "   9.06918278e-03  2.84650323e-01 -4.10305146e-02  1.88765880e-01]\n",
            " [-1.43004458e-01  9.30350179e-02  1.51221606e-01  2.37550676e-01\n",
            "   1.45645587e-01  3.01436469e-01  3.94285880e-03  4.84355540e+00\n",
            "   2.37812928e-02  1.78925206e-01 -3.73044533e-01  3.21024304e+00\n",
            "   2.30355642e-03  1.48708676e-01  2.54627307e+00 -6.30095496e-01\n",
            "  -1.61815326e-01  2.65214386e-01  1.92156703e-01  2.83264189e-01]\n",
            " [-1.97560253e-01 -1.08982062e-01 -2.90430501e-02  2.52275991e-01\n",
            "   9.52719539e-02  2.62357514e-01 -4.16602789e-01  1.15003548e-01\n",
            "  -2.82927344e-01  6.38923832e+00  7.67033060e-02  2.94324102e-01\n",
            "  -4.84020678e-01  3.68036788e+00  2.06511802e-01 -7.40599775e-02\n",
            "   1.70261080e-01 -9.47662383e-02  3.42094355e+00  3.35321557e+00]\n",
            " [ 2.80483807e+00 -4.47160842e-01 -8.31417155e-01 -5.82725619e-01\n",
            "  -5.08142770e-01  4.04741452e+00 -2.46224457e-01 -5.60101444e-01\n",
            "   2.74489686e+00 -2.36450168e-01 -4.30910124e-01 -2.90782861e-01\n",
            "   3.52361228e+00 -2.84418440e-01  3.47151868e+00  6.37340788e+00\n",
            "   3.07010613e+00 -3.42503150e-01  2.95129006e+00  3.55916094e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veU6FnEjZENA",
        "outputId": "d80b9920-3161-497d-fb47-dcd03898ac5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "env.reset()\n",
        "\n",
        "for episode in range(5):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    print(\"****************************************************\")\n",
        "    print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        \n",
        "        # Take the action (index) that have the maximum expected future reward given that state\n",
        "        action = encode_action(np.argmax(qtable[state,:]))\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        if done:\n",
        "            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "            env.render()\n",
        "            \n",
        "            # We print the number of step it took.\n",
        "            print(\"Number of steps\", step)\n",
        "            break\n",
        "        state = new_state\n",
        "    print(\"****************************************************\\n\")\n",
        "env.close()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "Total length of input instance: 5, step: 1\n",
            "==========================================\n",
            "Observation Tape    :  \u001b[42m \u001b[0mCDCDD  \n",
            "Output Tape         :   \u001b[41mE\u001b[0m\n",
            "Targets             :   CDCDD  \n",
            "\n",
            "Current reward      :   -0.500\n",
            "Cumulative reward   :   -0.500\n",
            "Action              :   Tuple(move over input: left,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: E)\n",
            "Number of steps 0\n",
            "****************************************************\n",
            "\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "Total length of input instance: 5, step: 1\n",
            "==========================================\n",
            "Observation Tape    :  \u001b[42m \u001b[0mDEDCD  \n",
            "Output Tape         :   \u001b[41mC\u001b[0m\n",
            "Targets             :   DEDCD  \n",
            "\n",
            "Current reward      :   -0.500\n",
            "Cumulative reward   :   -0.500\n",
            "Action              :   Tuple(move over input: left,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: C)\n",
            "Number of steps 0\n",
            "****************************************************\n",
            "\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "Total length of input instance: 5, step: 1\n",
            "==========================================\n",
            "Observation Tape    :  \u001b[42m \u001b[0mDBBAD  \n",
            "Output Tape         :   \u001b[41mC\u001b[0m\n",
            "Targets             :   DBBAD  \n",
            "\n",
            "Current reward      :   -0.500\n",
            "Cumulative reward   :   -0.500\n",
            "Action              :   Tuple(move over input: left,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: C)\n",
            "Number of steps 0\n",
            "****************************************************\n",
            "\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "Total length of input instance: 3, step: 2\n",
            "==========================================\n",
            "Observation Tape    :   \u001b[42mE\u001b[0mCA  \n",
            "Output Tape         :   E\u001b[41mA\u001b[0m\n",
            "Targets             :   ECA  \n",
            "\n",
            "Current reward      :   -0.500\n",
            "Cumulative reward   :   0.500\n",
            "Action              :   Tuple(move over input: right,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: A)\n",
            "Number of steps 1\n",
            "****************************************************\n",
            "\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "Total length of input instance: 3, step: 2\n",
            "==========================================\n",
            "Observation Tape    :   \u001b[42mB\u001b[0mED  \n",
            "Output Tape         :   B\u001b[41mA\u001b[0m\n",
            "Targets             :   BED  \n",
            "\n",
            "Current reward      :   -0.500\n",
            "Cumulative reward   :   0.500\n",
            "Action              :   Tuple(move over input: right,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: A)\n",
            "Number of steps 1\n",
            "****************************************************\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvUPcN9gakgE",
        "outputId": "5ee040c8-5c4a-40fb-a48d-f129cb8797c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "env.reset()\n",
        "env.render()"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total length of input instance: 3, step: 0\n",
            "==========================================\n",
            "Observation Tape    :   \u001b[42mA\u001b[0mBB  \n",
            "Output Tape         :   \n",
            "Targets             :   ABB  \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhYdfmexZpDK",
        "outputId": "399d5f38-61c0-4524-ff30-3099ba65c99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "state, reward, done, info = env.step((0, 1, 0))\n",
        "print(state, reward, done)\n",
        "env.render()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1.0 False\n",
            "Total length of input instance: 3, step: 9\n",
            "==========================================\n",
            "Observation Tape    :   A\u001b[42mB\u001b[0mB  \n",
            "Output Tape         :   \u001b[42mA\u001b[0m\n",
            "Targets             :   ABB  \n",
            "\n",
            "Current reward      :   1.000\n",
            "Cumulative reward   :   1.000\n",
            "Action              :   Tuple(move over input: left,\n",
            "                              write to the output tape: True,\n",
            "                              prediction: A)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzwdR4_0amnp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}